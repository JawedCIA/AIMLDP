{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1a4e9d-31bf-409d-bf2e-4d7fa3a79553",
   "metadata": {},
   "source": [
    "##Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dffaa40-ac2b-4ea9-8e92-e70bd2944dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "# Download the dataset\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Download successful\")\n",
    "else:\n",
    "    print(\"Failed to download the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4206e4-6c6e-4303-b8a3-7a6f8dcb60b3",
   "metadata": {},
   "source": [
    "Extract the downloaded zip dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82e5d81-0949-4a65-93c7-5f6967f1b947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction successful\n"
     ]
    }
   ],
   "source": [
    "# Extract the dataset\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"sms_spam_collection\")\n",
    "    print(\"Extraction successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09eb37-9eba-4307-a5ef-48996715d783",
   "metadata": {},
   "source": [
    "verify that the extraction was successful and to see what files were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca141ec-79ff-44c1-acf4-76fdea6d5463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['readme', 'SMSSpamCollection']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List the extracted files\n",
    "extracted_files = os.listdir(\"sms_spam_collection\")\n",
    "print(\"Extracted files:\", extracted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701db18-964b-440d-9f13-d927a5f1cf07",
   "metadata": {},
   "source": [
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb415c8-db5e-42c7-9c24-86e7b1a3f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\n",
    "    \"sms_spam_collection/SMSSpamCollection\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"label\", \"message\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178a0b1-0753-40ea-8eaa-b4a4d335826d",
   "metadata": {},
   "source": [
    "Here, we specify that the file is tab-separated (sep=\"\\t\"), and since the file does not contain a header row, we set header=None and provide column names manually using the names parameter.\n",
    "\n",
    "After loading the dataset, it is important to inspect it for basic information, missing values, and duplicates. This helps ensure that the data is clean and ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a7662c-9405-48d7-b174-db66bbb94a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- HEAD --------------------\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "-------------------- DESCRIBE --------------------\n",
      "       label                 message\n",
      "count   5572                    5572\n",
      "unique     2                    5169\n",
      "top      ham  Sorry, I'll call later\n",
      "freq    4825                      30\n",
      "-------------------- INFO --------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   label    5572 non-null   object\n",
      " 1   message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"-------------------- HEAD --------------------\")\n",
    "print(df.head())\n",
    "print(\"-------------------- DESCRIBE --------------------\")\n",
    "print(df.describe())\n",
    "print(\"-------------------- INFO --------------------\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955372b5-1239-4b39-8c00-4fecccd57d38",
   "metadata": {},
   "source": [
    "Checking for missing values is crucial to ensure that our dataset does not contain any incomplete entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9086daa7-d167-4340-9aa7-43366e254ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " label      0\n",
      "message    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75d502-c3fb-4ff6-8a89-eb369066cb30",
   "metadata": {},
   "source": [
    "Duplicate entries can skew the results of our analysis, so it's important to identify and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571fcbc5-8f61-4a10-8c23-60f968baeddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate entries: 403\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "print(\"Duplicate entries:\", df.duplicated().sum())\n",
    "\n",
    "# Remove duplicates if any\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21889d96-2a45-421d-acd1-4355e29d370e",
   "metadata": {},
   "source": [
    "The duplicated method returns a boolean Series indicating whether each row is a duplicate or not. The sum method counts the number of True values, giving us the total number of duplicate entries. We then use the drop_duplicates method to remove these duplicates from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd10e906-a3ff-436c-9474-672f4da31dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- DESCRIBE --------------------\n",
      "       label                     message\n",
      "count   5169                        5169\n",
      "unique     2                        5169\n",
      "top      ham  Rofl. Its true to its name\n",
      "freq    4516                           1\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------- DESCRIBE --------------------\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c055038-2a4d-41ab-82a3-8d091b2817b0",
   "metadata": {},
   "source": [
    "** Preprocessing the Spam Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5daac013-87c4-4ab3-a452-817efc3d6f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ANY PREPROCESSING ===\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jawed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jawed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jawed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "# Force clean re-download of everything required\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "print(\"=== BEFORE ANY PREPROCESSING ===\") \n",
    "print(df.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865f14d1-8882-481d-b846-ea1ef4437b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER LOWERCASING ===\n",
      "0    go until jurong point, crazy.. available only ...\n",
      "1                        ok lar... joking wif u oni...\n",
      "2    free entry in 2 a wkly comp to win fa cup fina...\n",
      "3    u dun say so early hor... u c already then say...\n",
      "4    nah i don't think he goes to usf, he lives aro...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert all message text to lowercase\n",
    "df[\"message\"] = df[\"message\"].str.lower()\n",
    "print(\"\\n=== AFTER LOWERCASING ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4aa0594-c21f-4e08-9ea3-b854d8e75031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\n",
      "0    go until jurong point crazy available only in ...\n",
      "1                              ok lar joking wif u oni\n",
      "2    free entry in  a wkly comp to win fa cup final...\n",
      "3          u dun say so early hor u c already then say\n",
      "4    nah i dont think he goes to usf he lives aroun...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Remove non-essential punctuation and numbers, keep useful symbols like $ and !\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
    "print(\"\\n=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3052d484-22f3-4e0c-b420-25c5ef69a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER TOKENIZATION ===\n",
      "0    [go, until, jurong, point, crazy, available, o...\n",
      "1                       [ok, lar, joking, wif, u, oni]\n",
      "2    [free, entry, in, a, wkly, comp, to, win, fa, ...\n",
      "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
      "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split each message into individual tokens\n",
    "df[\"message\"] = df[\"message\"].apply(word_tokenize)\n",
    "print(\"\\n=== AFTER TOKENIZATION ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1bd4090-1516-4979-a85e-bcd4f52b3931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING STOP WORDS ===\n",
      "0    [go, jurong, point, crazy, available, bugis, n...\n",
      "1                       [ok, lar, joking, wif, u, oni]\n",
      "2    [free, entry, wkly, comp, win, fa, cup, final,...\n",
      "3        [u, dun, say, early, hor, u, c, already, say]\n",
      "4    [nah, dont, think, goes, usf, lives, around, t...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define a set of English stop words and remove them from the tokens\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(\"\\n=== AFTER REMOVING STOP WORDS ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d725cdb4-947c-40a5-95a4-0cb913ca2de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER STEMMING ===\n",
      "0    [go, jurong, point, crazi, avail, bugi, n, gre...\n",
      "1                         [ok, lar, joke, wif, u, oni]\n",
      "2    [free, entri, wkli, comp, win, fa, cup, final,...\n",
      "3        [u, dun, say, earli, hor, u, c, alreadi, say]\n",
      "4    [nah, dont, think, goe, usf, live, around, tho...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Stem each token to reduce words to their base form\n",
    "stemmer = PorterStemmer()\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "print(\"\\n=== AFTER STEMMING ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5244e889-dee8-40f6-8ea9-8319aa02e453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER JOINING TOKENS BACK INTO STRINGS ===\n",
      "0    go jurong point crazi avail bugi n great world...\n",
      "1                                ok lar joke wif u oni\n",
      "2    free entri wkli comp win fa cup final tkt st m...\n",
      "3                  u dun say earli hor u c alreadi say\n",
      "4            nah dont think goe usf live around though\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Rejoin tokens into a single string for feature extraction\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(x))\n",
    "print(\"\\n=== AFTER JOINING TOKENS BACK INTO STRINGS ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b245166-f98f-4da1-bf47-69b32ac625f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the message column\n",
    "X = vectorizer.fit_transform(df[\"message\"])\n",
    "\n",
    "# Labels (target variable)\n",
    "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)  # Converting labels to 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30edb719-ed91-4c5c-b4fa-8ca8ef136a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       1\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "5567    1\n",
      "5568    0\n",
      "5569    0\n",
      "5570    0\n",
      "5571    0\n",
      "Name: label, Length: 5169, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8179b94-d206-4060-b996-fe2d2688bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build the pipeline by combining vectorization and classification\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c9f158-3949-49b9-893c-e2e754815c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'classifier__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "# Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=10,\n",
    "    scoring=\"f1\"\n",
    ")\n",
    "\n",
    "# Fit the grid search on the full dataset\n",
    "grid_search.fit(df[\"message\"], y)\n",
    "\n",
    "# Extract the best model identified by the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2de050-539c-4ffe-a2a0-89fb473309db",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36109fc7-87bd-4613-9ec4-a3c77cebf3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example SMS messages for evaluation\n",
    "new_messages = [\n",
    "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
    "    \"Hey, are we still meeting up for lunch today?\",\n",
    "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
    "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
    "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
    "    \"Act now! Limited time offer to earn big rewards!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5d82b41-d520-40de-b9a7-67452e21b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Preprocess function that mirrors the training-time preprocessing\n",
    "def preprocess_message(message):\n",
    "    message = message.lower()\n",
    "    message = re.sub(r\"[^a-z\\s$!]\", \"\", message)\n",
    "    tokens = word_tokenize(message)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80cbc7ee-a228-4d11-a0a7-def754a8abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and vectorize messages\n",
    "processed_messages = [preprocess_message(msg) for msg in new_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cd0d4ad-b030-48f3-8a52-78b3277d2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform preprocessed messages into feature vectors\n",
    "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "675ca532-9c00-4a65-8fac-eb5e91716d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the trained classifier\n",
    "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
    "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07c86570-bbbc-498b-aa18-2148bb577a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
      "Prediction: Spam\n",
      "Spam Probability: 1.00\n",
      "Not-Spam Probability: 0.00\n",
      "--------------------------------------------------\n",
      "Message: Hey, are we still meeting up for lunch today?\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.00\n",
      "Not-Spam Probability: 1.00\n",
      "--------------------------------------------------\n",
      "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
      "Prediction: Spam\n",
      "Spam Probability: 0.94\n",
      "Not-Spam Probability: 0.06\n",
      "--------------------------------------------------\n",
      "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.00\n",
      "Not-Spam Probability: 1.00\n",
      "--------------------------------------------------\n",
      "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
      "Prediction: Spam\n",
      "Spam Probability: 1.00\n",
      "Not-Spam Probability: 0.00\n",
      "--------------------------------------------------\n",
      "Message: Act now! Limited time offer to earn big rewards!\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.01\n",
      "Not-Spam Probability: 0.99\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display predictions and probabilities for each evaluated message\n",
    "for i, msg in enumerate(new_messages):\n",
    "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
    "    spam_probability = prediction_probabilities[i][1]  # Probability of being spam\n",
    "    ham_probability = prediction_probabilities[i][0]   # Probability of being not spam\n",
    "    \n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
    "    print(f\"Not-Spam Probability: {ham_probability:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ee635-bc01-4cca-930b-421c8f06e21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7155f0aa-83e9-4009-aa4b-c1ed7f6cea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5f9fd82-2295-4924-8148-a3d461d9d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'spam_detection_model.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0c4f879-ea3c-4dfe-8f76-7cf36620d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to spam_detection_model.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Save the trained model to a file for future use\n",
    "\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05d7fcbd-4c9d-4b97-b324-46ed721ea6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(model_filename)\n",
    "predictions = loaded_model.predict(new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "394af94b-2e91-47cf-86fb-37c0753ba1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e65aec3-c97d-4547-b77b-e9a236401067",
   "metadata": {},
   "outputs": [],
   "source": [
    "message01 = [\"\"\"Hey Md Jawed, Weâ€™re thrilled to have you join the Outskill community! To get started,\n",
    "just follow the steps in your welcome email and explore all the learning paths we offer.\"\"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f957ece9-9e97-49e7-88e7-ecbe41723790",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(model_filename)\n",
    "predictions = loaded_model.predict(message01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f7dd700-0cf4-48fc-9635-0b9cf33b247b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bde1a5-6c1c-4d89-9ebd-210fa4a50ded",
   "metadata": {},
   "source": [
    "ðŸ“¤ Uploading a File using Python Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84707423-f0d4-4c2a-9195-d56d6ae91d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"status\": \"success\",\n",
      "    \"filename\": \"spam_detection_model.joblib\",\n",
      "    \"path\": \"uploaded_models\\\\spam_detection_model.joblib\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL of the API endpoint\n",
    "url = \"http://localhost:8000/api/upload\"\n",
    "\n",
    "# Path to the model file you want to upload\n",
    "model_file_path = \"spam_detection_model.joblib\"\n",
    "\n",
    "# Open the file in binary mode and send the POST request\n",
    "with open(model_file_path, \"rb\") as model_file:\n",
    "    files = {\"model\": model_file}\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "# Pretty print the response from the server\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8bd4cd-9abe-4255-9637-d6ee9067ff36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
